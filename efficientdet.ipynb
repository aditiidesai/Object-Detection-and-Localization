{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8-zC43HZ78n",
        "outputId": "5869f00d-d5d5-4bdc-b473-9e2db8600d38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.0.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# configuring the path of Kaggle.json file\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "LwdFLzMAaBkZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# daatset api\n",
        "!kaggle datasets download -d thepbordin/indoor-object-detection"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyWr6foGaPdm",
        "outputId": "3307f4a0-2ec6-4e38-b84a-ba3290de3194"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading indoor-object-detection.zip to /content\n",
            " 98% 359M/367M [00:04<00:00, 104MB/s]\n",
            "100% 367M/367M [00:04<00:00, 91.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting the compessed Dataset\n",
        "from zipfile import ZipFile\n",
        "dataset = '/content/indoor-object-detection.zip'\n",
        "\n",
        "with ZipFile(dataset,'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('The dataset is extracted')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxN3ZtD9aST1",
        "outputId": "b4845b4c-b6a0-4927-90ea-6c7d642298a7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dataset is extracted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import random_split\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "random_seed = 213\n",
        "torch.manual_seed(random_seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwAyoXHmaUnL",
        "outputId": "de0ce981-37da-48f5-88fa-3488e922555b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7b8a000466b0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data_dir  = '/content/train/images'\n",
        "\n",
        "#classes = os.listdir(data_dir)\n",
        "classes={0:\"door\",1:\"cabinetDoor\",2:\"refrigeratorDoor\",3:\"window\",4:\"chair\",5:\"table\",\n",
        "       6:\"cabinet\",7:\"couch\",8:\"openedDoor\",9:\"pole\"}\n",
        "print(classes)\n",
        "print(f\"length: {len(classes)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mK1jqjBBaW7K",
        "outputId": "069fa15b-457c-4fd7-b196-e1ff67178595"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'door', 1: 'cabinetDoor', 2: 'refrigeratorDoor', 3: 'window', 4: 'chair', 5: 'table', 6: 'cabinet', 7: 'couch', 8: 'openedDoor', 9: 'pole'}\n",
            "length: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Count image files\n",
        "image_files = os.listdir('train/images')\n",
        "num_image_files = len(image_files)\n",
        "\n",
        "# Count label files\n",
        "label_files = os.listdir('train/labels')\n",
        "num_label_files = len(label_files)\n",
        "\n",
        "print(f'Number of image files: {num_image_files}')\n",
        "print(f'Number of label files: {num_label_files}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36W-nyv-5Z82",
        "outputId": "5edd34ab-8f06-4281-90b4-d2d75c33aba9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of image files: 1012\n",
            "Number of label files: 1008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_files = os.listdir('train/images')\n",
        "label_files = os.listdir('train/labels')\n",
        "\n",
        "# Remove file extensions\n",
        "image_names = [os.path.splitext(filename)[0] for filename in image_files]\n",
        "label_names = [os.path.splitext(filename)[0] for filename in label_files]\n",
        "\n",
        "print(f'Number of image files: {len(image_names)}')\n",
        "print(f'Number of label files: {len(label_names)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAkcVoIq6L7r",
        "outputId": "fe3c2e18-67db-4fc9-8c03-4051c99f4b9a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of image files: 1012\n",
            "Number of label files: 1008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extra_images = [image_name for image_name in image_names if image_name not in label_names]\n"
      ],
      "metadata": {
        "id": "JdmSUBWD5sPM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "images_directory = 'train/images'\n",
        "\n",
        "for image_name in extra_images:\n",
        "    image_file = os.path.join(images_directory, f'{image_name}.jpg')  # Adjust the file extension as needed\n",
        "    os.remove(image_file)\n"
      ],
      "metadata": {
        "id": "NEH8ajW355Pl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_files = os.listdir('train/images')\n",
        "label_files = os.listdir('train/labels')\n",
        "\n",
        "# Remove file extensions\n",
        "image_names = [os.path.splitext(filename)[0] for filename in image_files]\n",
        "label_names = [os.path.splitext(filename)[0] for filename in label_files]\n",
        "\n",
        "print(f'Number of image files: {len(image_names)}')\n",
        "print(f'Number of label files: {len(label_names)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKD5Cvsq57ur",
        "outputId": "8b74d9f2-4f1a-4e52-d1c8-3819b7dac82a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of image files: 1008\n",
            "Number of label files: 1008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Input, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "# Define the directory paths\n",
        "train_images_directory = 'train/images'\n",
        "train_labels_directory = 'train/labels'\n",
        "\n",
        "# Load and preprocess labels from text files\n",
        "def load_labels(directory):\n",
        "    labels = []\n",
        "\n",
        "    # Iterate through all text files in the directory\n",
        "    for filename in glob.glob(os.path.join(directory, '*.txt')):\n",
        "        with open(filename, 'r') as file:\n",
        "            lines = file.readlines()\n",
        "\n",
        "        annotations = []\n",
        "        for line in lines:\n",
        "            parts = line.strip().split(' ')\n",
        "            class_label = int(parts[0])\n",
        "            x_center = float(parts[1])\n",
        "            y_center = float(parts[2])\n",
        "            width = float(parts[3])\n",
        "            height = float(parts[4])\n",
        "\n",
        "            annotation = {\n",
        "                'class_label': class_label,\n",
        "                'bounding_box': (x_center, y_center, width, height)\n",
        "            }\n",
        "\n",
        "            annotations.append(annotation)\n",
        "\n",
        "        labels.append(annotations)\n",
        "\n",
        "    return labels\n",
        "\n",
        "# Load image filenames\n",
        "#image_files = glob.glob(os.path.join(train_images_directory, '*.jpg'))\n",
        "\n",
        "# Load EfficientDet model\n",
        "def efficientdet_model(num_classes):\n",
        "    base_model = EfficientNetB0(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
        "    base_model.trainable = False\n",
        "    x = base_model.output\n",
        "    x = Conv2D(256, (3, 3), activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    predictions = Dense(num_classes, activation='sigmoid')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    return model\n",
        "\n",
        "# Create an instance of the EfficientDet model\n",
        "num_classes = 10  # Replace with the actual number of classes\n",
        "model = efficientdet_model(num_classes)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess labels\n",
        "train_labels = load_labels(train_labels_directory)\n",
        "\n",
        "print(len(train_labels))\n",
        "print(len(image_files))\n",
        "\n",
        "# Ensure that the number of samples in labels matches the number of samples in image_files\n",
        "assert len(train_labels) == len(image_files), \"Number of labels and images must match.\"\n",
        "\n",
        "# Load and preprocess images and labels\n",
        "X_train = []  # To store preprocessed images\n",
        "Y_train = []  # To store corresponding labels\n",
        "\n",
        "for image_file, labels in (image_files, train_labels):\n",
        "    # Load and preprocess the image\n",
        "    img = load_img(image_file, target_size=(224, 224))\n",
        "    img_array = img_to_array(img) / 255.0  # Normalize the image\n",
        "    X_train.append(img_array)\n",
        "\n",
        "    # Create a label vector for this image\n",
        "    label_vector = [0] * num_classes\n",
        "    for annotation in labels:\n",
        "        class_label = annotation['class_label']\n",
        "        label_vector[class_label] = 1\n",
        "    Y_train.append(label_vector)\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "Y_train = np.array(Y_train)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, Y_train, epochs=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "gsbnvwXVHtzP",
        "outputId": "b2826411-71ad-422d-f0de-bd629c3dbd72"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1008\n",
            "1008\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-1f635b4f4ef0>\u001b[0m in \u001b[0;36m<cell line: 80>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0mY_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# To store corresponding labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimage_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0;31m# Load and preprocess the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Input, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Define the directory paths\n",
        "train_images_directory = 'train/images'\n",
        "train_labels_directory = 'train/labels'\n",
        "\n",
        "# Load and preprocess labels from text files\n",
        "def load_labels(directory):\n",
        "    labels = []\n",
        "\n",
        "    # Iterate through all text files in the directory\n",
        "    for filename in glob.glob(os.path.join(directory, '*.txt')):\n",
        "        with open(filename, 'r') as file:\n",
        "            lines = file.readlines()\n",
        "\n",
        "        annotations = []\n",
        "        for line in lines:\n",
        "            parts = line.strip().split(' ')\n",
        "            class_label = int(parts[0])\n",
        "            x_center = float(parts[1])\n",
        "            y_center = float(parts[2])\n",
        "            width = float(parts[3])\n",
        "            height = float(parts[4])\n",
        "\n",
        "            annotation = {\n",
        "                'class_label': class_label,\n",
        "                'bounding_box': (x_center, y_center, width, height)\n",
        "            }\n",
        "\n",
        "            annotations.append(annotation)\n",
        "\n",
        "        labels.append(annotations)\n",
        "\n",
        "    return labels\n",
        "\n",
        "# Load and preprocess image data using ImageDataGenerator\n",
        "image_size = (224, 224)\n",
        "batch_size = 32\n",
        "\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_images_directory,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=None,  # Set to None for custom labels\n",
        "    shuffle=False  # Important: Do not shuffle images for correct label matching\n",
        ")\n",
        "\n",
        "# Load EfficientDet model\n",
        "def efficientdet_model(num_classes):\n",
        "    base_model = EfficientNetB0(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
        "    base_model.trainable = False\n",
        "    x = base_model.output\n",
        "    x = Conv2D(256, (3, 3), activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    predictions = Dense(num_classes, activation='sigmoid')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    return model\n",
        "\n",
        "# Create an instance of the EfficientDet model\n",
        "num_classes = 10  # Replace with the actual number of classes\n",
        "model = efficientdet_model(num_classes)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess labels\n",
        "train_labels = load_labels(train_labels_directory)\n",
        "\n",
        "print(len(train_labels))\n",
        "print(len(image_files))\n",
        "print(len(train_generator.filenames))\n",
        "# Ensure that the number of samples in labels matches the number of samples in the generator\n",
        "#assert len(train_labels) == len(train_generator.filenames), \"Number of labels and images must match.\"\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_generator, steps_per_epoch=len(train_generator), epochs=10)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "sr9mrz8n4oOR",
        "outputId": "73407546-8a20-432c-be68-03e0d814dcc5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 images belonging to 0 classes.\n",
            "1008\n",
            "1008\n",
            "0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-7d87a7a0adda>\u001b[0m in \u001b[0;36m<cell line: 85>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/preprocessing/image.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    104\u001b[0m                 \u001b[0;34m\"Asked to retrieve element {idx}, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0;34m\"but the Sequence \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Asked to retrieve element 0, but the Sequence has length 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_images='/content/train/images'\n",
        "val_images='/content/valid/images'\n",
        "test_images='/content/test/images'\n",
        "\n",
        "train_labels='/content/train/labels'\n",
        "val_labels='/content/valid/labels'\n",
        "test_labels='/content/test/labels'"
      ],
      "metadata": {
        "id": "azK8GqSXauQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "# Directory containing the annotation text files\n",
        "labels_directory = 'train/labels'\n",
        "\n",
        "# List to store all annotations from all files\n",
        "train_labels = []\n",
        "\n",
        "# Iterate through all text files in the directory\n",
        "for filename in glob.glob(os.path.join(labels_directory, '*.txt')):\n",
        "    with open(filename, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    annotations = []\n",
        "    for line in lines:\n",
        "        parts = line.strip().split(' ')\n",
        "        class_label = int(parts[0])\n",
        "        x_center = float(parts[1])\n",
        "        y_center = float(parts[2])\n",
        "        width = float(parts[3])\n",
        "        height = float(parts[4])\n",
        "\n",
        "        annotation = {\n",
        "            'class_label': class_label,\n",
        "            'bounding_box': (x_center, y_center, width, height)\n",
        "        }\n",
        "\n",
        "        annotations.append(annotation)\n",
        "\n",
        "    # Extend the train_labels list with annotations from the current file\n",
        "    train_labels.extend(annotations)\n",
        "\n",
        "#\n",
        "\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Directory containing the annotation text files\n",
        "labels_directory = 'test/labels'\n",
        "\n",
        "# List to store all annotations\n",
        "test_labels = []\n",
        "\n",
        "# Iterate through all text files in the directory\n",
        "for filename in glob.glob(os.path.join(labels_directory, '/*.txt')):\n",
        "    with open(filename, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    annotations = []\n",
        "    for line in lines:\n",
        "        parts = line.strip().split(' ')\n",
        "        class_label = int(parts[0])\n",
        "        x_center = float(parts[1])\n",
        "        y_center = float(parts[2])\n",
        "        width = float(parts[3])\n",
        "        height = float(parts[4])\n",
        "\n",
        "        annotation = {\n",
        "            'class_label': class_label,\n",
        "            'bounding_box': (x_center, y_center, width, height)\n",
        "        }\n",
        "\n",
        "        annotations.append(annotation)\n",
        "\n",
        "    test_labels.extend(annotations)\n",
        "\n",
        "# Now, train_labels is a single list containing all annotations from all text files.\n",
        "\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Directory containing the annotation text files\n",
        "labels_directory = 'valid/labels'\n",
        "\n",
        "# List to store all annotations\n",
        "val_labels = []\n",
        "\n",
        "# Iterate through all text files in the directory\n",
        "for filename in glob.glob(os.path.join(labels_directory, '*.txt')):\n",
        "    with open(filename, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    annotations = []\n",
        "    for line in lines:\n",
        "        parts = line.strip().split(' ')\n",
        "        class_label = int(parts[0])\n",
        "        x_center = float(parts[1])\n",
        "        y_center = float(parts[2])\n",
        "        width = float(parts[3])\n",
        "        height = float(parts[4])\n",
        "\n",
        "        annotation = {\n",
        "            'class_label': class_label,\n",
        "            'bounding_box': (x_center, y_center, width, height)\n",
        "        }\n",
        "\n",
        "        annotations.append(annotation)\n",
        "\n",
        "    val_labels.extend(annotations)\n",
        "\n",
        "# Now, train_labels is a single list containing all annotations from all text files.\n"
      ],
      "metadata": {
        "id": "u2xBuBnyy9ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define data generators for train, validation, and test sets\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Create data generators\n",
        "train_generator = train_datagen.flow_from_directory('train/images', target_size=(224, 224), batch_size=32, class_mode=None)\n",
        "val_generator = val_datagen.flow_from_directory('valid/images', target_size=(224, 224), batch_size=32, class_mode=None)\n",
        "test_generator = test_datagen.flow_from_directory('test/images', target_size=(224, 224), batch_size=32, class_mode=None)\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_generator, epochs=10, validation_data=val_generator)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(test_generator)\n",
        "\n",
        "# Perform object detection and localization on a test image\n",
        "image = load_image('test.jpg')  # Implement load_image function to load and preprocess an image\n",
        "predictions = model.predict(image)\n"
      ],
      "metadata": {
        "id": "G-QF14Pf31R0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Input, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Define the EfficientDet architecture\n",
        "def efficientdet_model():\n",
        "    # Use EfficientNetB0 as the base model\n",
        "    base_model = EfficientNetB0(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
        "\n",
        "    # Freeze the base model layers\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # Add additional layers for object detection\n",
        "    x = base_model.output\n",
        "    x = Conv2D(256, (3, 3), activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    predictions = Dense(10, activation='sigmoid')(x)\n",
        "\n",
        "    # Create the model\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create an instance of the EfficientDet model\n",
        "model = efficientdet_model()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_images, train_labels, epochs=10, batch_size=32, validation_data=(val_images, val_labels))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(test_images, test_labels)\n",
        "\n",
        "# Perform object detection and localization on a test image\n",
        "image = load_image('test.jpg')\n",
        "predictions = model.predict(image)"
      ],
      "metadata": {
        "id": "iEO_t7iQad7f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}